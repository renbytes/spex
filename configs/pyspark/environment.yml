name: pyspark-analysis-env
channels:
  - conda-forge
  - defaults
dependencies:
  # Python and Java (required for Spark)
  - python=3.11
  - openjdk=17
  
  # Core data science stack
  - pandas>=1.5.0
  - numpy>=1.21.0
  - scipy>=1.7.0
  
  # Visualization
  - matplotlib>=3.5.0
  - seaborn>=0.11.0
  - plotly>=5.0.0
  
  # Jupyter ecosystem with Spark support
  - jupyter
  - jupyterlab
  - ipykernel
  - ipywidgets
  
  # Performance and data formats
  - pyarrow>=12.0.0
  - fastparquet
  - numba
  
  # Development tools
  - pytest
  - pytest-cov
  - black
  - flake8
  - mypy
  - pre-commit
  
  # System utilities
  - make
  - git
  
  # Spark and big data tools
  - pip
  - pip:
    # PySpark
    - pyspark>=3.4.0
    - py4j>=0.10.9
    - findspark>=2.0.0
    
    # Delta Lake
    - delta-spark>=2.4.0
    
    # Data validation for big data
    - great-expectations>=0.15.0
    
    # Configuration management
    - python-dotenv>=1.0.0
    - pydantic>=2.0.0
    - pydantic-settings>=2.0.0
    
    # Logging
    - loguru>=0.7.0
    
    # Cloud storage (optional)
    - boto3>=1.28.0  # AWS
    - s3fs>=2023.6.0  # S3 filesystem
    
    # ML libraries compatible with Spark
    - mlflow>=2.6.0
    - scikit-learn>=1.3.0